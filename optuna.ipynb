{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84e9aef7",
      "metadata": {
        "id": "84e9aef7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timz815/360-NLP-Project/blob/main/optuna.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, tempfile\n",
        "# wipe any old compiled pieces\n",
        "shutil.rmtree(os.path.expanduser(\"~/unsloth_compiled_cache\"), ignore_errors=True)\n",
        "\n",
        "# give Inductor a brand-new temp directory\n",
        "os.environ[\"TMPDIR\"] = tempfile.mkdtemp(prefix=\"torchinductor_\")\n",
        "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = os.environ[\"TMPDIR\"]\n",
        "# disable the dynamo compile that crashes on missing tmp files\n",
        "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
        "\n",
        "# Unsloth settings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"UNSLOTH_DISABLE_FUSED_LOSS\"] = \"1\"\n",
        "os.environ[\"UNSLOTH_FREE_GB\"] = \"4\"\n",
        "os.environ[\"UNSLOTH_DISABLE_COMPILED_CACHE\"] = \"1\""
      ],
      "metadata": {
        "id": "GFAbIwuJEwgj"
      },
      "id": "GFAbIwuJEwgj",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc, optuna\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import Dataset\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WePdTQ8-Ewpb",
        "outputId": "8f99f005-5789-461a-e8ad-d08c0273cb81"
      },
      "id": "WePdTQ8-Ewpb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
            "  warnings.warn(f\"Failed to find {binary}\")\n",
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
            "  warnings.warn(f\"Failed to find {binary}\")\n",
            "W1205 16:06:41.258000 6264 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
            "[tensorflow|WARNING]From C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_path = r\"C:\\Users\\timot\\Downloads\\nlp training\\movie_dialogue.jsonl\"\n",
        "data = [json.loads(line) for line in open(jsonl_path, encoding=\"utf-8\")]\n",
        "ds = Dataset.from_list(data)\n",
        "\n",
        "splits = ds.train_test_split(test_size=0.2, seed=42)\n",
        "test_ds = splits[\"test\"]\n",
        "train_val = splits[\"train\"]\n",
        "\n",
        "val_splits = train_val.train_test_split(test_size=0.125, seed=42)\n",
        "train_ds = val_splits[\"train\"]\n",
        "val_ds   = val_splits[\"test\"]\n",
        "\n",
        "print(\"Train:\", len(train_ds))\n",
        "print(\"Val:\",   len(val_ds))\n",
        "print(\"Test:\",  len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byrc8_MREwsb",
        "outputId": "c5fbb633-fa57-4a27-beb8-f5a4023fe1a7"
      },
      "id": "Byrc8_MREwsb",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 6608\n",
            "Val: 944\n",
            "Test: 1889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\"\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "print(\"Base model in VRAM:\", torch.cuda.memory_allocated()/1024**3, \"GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-u1i6aQEwvD",
        "outputId": "03502eef-a371-4c40-f4e8-c0cc836171df"
      },
      "id": "x-u1i6aQEwvD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
            "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE_TORCH}:{i}\") for i in range(n_gpus)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.4: Fast Qwen3 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Base model in VRAM: 3.3413352966308594 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_example(ex, tokenizer):\n",
        "    messages = [\n",
        "        {\"role\": \"user\",      \"content\": ex[\"chinese\"]},\n",
        "        {\"role\": \"assistant\", \"content\": ex[\"english\"]},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "def tokenize_example(ex, tokenizer, max_length=2048):\n",
        "    t = tokenizer(ex[\"text\"], truncation=True, max_length=max_length)\n",
        "    return {\"input_ids\": t[\"input_ids\"], \"attention_mask\": t[\"attention_mask\"]}\n",
        "\n",
        "train_ds = train_ds.map(format_example, fn_kwargs={\"tokenizer\": tokenizer}) \\\n",
        "                   .map(tokenize_example, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "val_ds   = val_ds.map(format_example, fn_kwargs={\"tokenizer\": tokenizer}) \\\n",
        "                 .map(tokenize_example, fn_kwargs={\"tokenizer\": tokenizer})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2rKZqLGEwxr",
        "outputId": "57fa66ce-b90d-4f7e-bb05-780116b5db50"
      },
      "id": "I2rKZqLGEwxr",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6608/6608 [00:01<00:00, 3745.01 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6608/6608 [00:01<00:00, 3983.72 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 944/944 [00:00<00:00, 3037.80 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 944/944 [00:00<00:00, 4765.45 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    lr   = trial.suggest_float(\"lr\", 3e-4, 5e-4, log=True)          # focus on best range\n",
        "    bs   = trial.suggest_categorical(\"per_device_train_batch_size\", [1, 2])\n",
        "    ga   = trial.suggest_categorical(\"gradient_accumulation_steps\", [2, 4])\n",
        "    r    = trial.suggest_categorical(\"lora_r\", [8, 16])\n",
        "    alpha = trial.suggest_categorical(\"lora_alpha\", [16, 32])\n",
        "    # fixed small warm-up / steps\n",
        "    warmup = 10\n",
        "    max_steps = 100\n",
        "\n",
        "    print(f\"\\nTrial {trial.number}  lr={lr:.2e}  r={r}  bs={bs}  ga={ga}\")\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        base_model,\n",
        "        r=r, lora_alpha=alpha,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_dropout=0.0, bias=\"none\",\n",
        "        use_gradient_checkpointing=True,\n",
        "    )\n",
        "\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size=bs,\n",
        "        gradient_accumulation_steps=ga,\n",
        "        warmup_steps=warmup,\n",
        "        max_steps=max_steps,\n",
        "        learning_rate=lr,\n",
        "        logging_steps=20,\n",
        "        output_dir=f\"./fast_trial_{trial.number}\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=20,\n",
        "        save_strategy=\"no\",\n",
        "        load_best_model_at_end=False,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        args=args,\n",
        "    )\n",
        "    trainer.train()\n",
        "    loss = trainer.evaluate().get(\"eval_loss\", 9999.0)\n",
        "\n",
        "    del trainer, model\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "jICOBrDpEw0b"
      },
      "id": "jICOBrDpEw0b",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    sampler=optuna.samplers.TPESampler(seed=42)\n",
        ")\n",
        "study.optimize(objective, n_trials=6, gc_after_trial=True)\n",
        "\n",
        "print(\"\\n===== BEST =====\")\n",
        "print(\"value :\", study.best_value)\n",
        "print(\"params:\", study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ohFzVDdGEw3r",
        "outputId": "3db45860-db4c-4c06-a1f4-c34300a34cf6"
      },
      "id": "ohFzVDdGEw3r",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 16:07:39,059] A new study created in memory with name: no-name-0f155d7c-420b-4df8-ac2a-b8ee915a6865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 0  lr=3.63e-04  r=8  bs=1  ga=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.4 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,608 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 16,515,072 of 4,038,983,168 (0.41% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 06:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.653700</td>\n",
              "      <td>2.220310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.914800</td>\n",
              "      <td>1.935260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.622700</td>\n",
              "      <td>1.743312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.606600</td>\n",
              "      <td>1.735350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.909300</td>\n",
              "      <td>1.708796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='236' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [236/236 00:37]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 16:14:39,871] Trial 0 finished with value: 1.7087962627410889 and parameters: {'lr': 0.00036325576193223686, 'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 2, 'lora_r': 8, 'lora_alpha': 16}. Best is trial 0 with value: 1.7087962627410889.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 1  lr=4.31e-04  r=16  bs=2  ga=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,608 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 05:57, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.984000</td>\n",
              "      <td>1.829086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.705400</td>\n",
              "      <td>1.765684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.802200</td>\n",
              "      <td>1.714928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.644400</td>\n",
              "      <td>1.685344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.731400</td>\n",
              "      <td>1.656623</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='236' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [236/236 00:36]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 16:21:21,998] Trial 1 finished with value: 1.656623363494873 and parameters: {'lr': 0.00043073114022763095, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 2, 'lora_r': 16, 'lora_alpha': 32}. Best is trial 1 with value: 1.656623363494873.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 2  lr=3.74e-04  r=16  bs=2  ga=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,608 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 08:17, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.379300</td>\n",
              "      <td>2.142414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.835500</td>\n",
              "      <td>1.726653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.719700</td>\n",
              "      <td>1.667916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.683400</td>\n",
              "      <td>1.645233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.685400</td>\n",
              "      <td>1.636911</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='236' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [236/236 00:35]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 16:30:25,211] Trial 2 finished with value: 1.6369105577468872 and parameters: {'lr': 0.00037406555329177457, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 4, 'lora_r': 16, 'lora_alpha': 16}. Best is trial 2 with value: 1.6369105577468872.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 3  lr=3.90e-04  r=16  bs=1  ga=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,608 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 05:58, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.616400</td>\n",
              "      <td>2.196567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.903700</td>\n",
              "      <td>1.797203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.605800</td>\n",
              "      <td>1.743973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.605900</td>\n",
              "      <td>1.737537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.891700</td>\n",
              "      <td>1.702291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='236' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [236/236 00:38]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 16:37:09,690] Trial 3 finished with value: 1.7022908926010132 and parameters: {'lr': 0.0003901247666389379, 'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 2, 'lora_r': 16, 'lora_alpha': 16}. Best is trial 2 with value: 1.6369105577468872.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 4  lr=3.51e-04  r=8  bs=2  ga=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,608 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 16,515,072 of 4,038,983,168 (0.41% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 05:45, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.593900</td>\n",
              "      <td>2.200164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.799400</td>\n",
              "      <td>1.760895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.786300</td>\n",
              "      <td>1.716440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.638900</td>\n",
              "      <td>1.693129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.738700</td>\n",
              "      <td>1.673455</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='236' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [236/236 00:36]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 16:43:39,696] Trial 4 finished with value: 1.6734552383422852 and parameters: {'lr': 0.00035050921365405055, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 2, 'lora_r': 8, 'lora_alpha': 16}. Best is trial 2 with value: 1.6369105577468872.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial 5  lr=4.21e-04  r=8  bs=2  ga=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,608 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 16,515,072 of 4,038,983,168 (0.41% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 05:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.409800</td>\n",
              "      <td>2.115174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.745300</td>\n",
              "      <td>1.755527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.783600</td>\n",
              "      <td>1.707507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.633800</td>\n",
              "      <td>1.683467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.733600</td>\n",
              "      <td>1.663672</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='236' max='236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [236/236 00:35]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 16:50:11,547] Trial 5 finished with value: 1.6636719703674316 and parameters: {'lr': 0.0004208244796565418, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 2, 'lora_r': 8, 'lora_alpha': 16}. Best is trial 2 with value: 1.6369105577468872.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== BEST =====\n",
            "value : 1.6369105577468872\n",
            "params: {'lr': 0.00037406555329177457, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 4, 'lora_r': 16, 'lora_alpha': 16}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}