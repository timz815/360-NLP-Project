{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "230a5984",
      "metadata": {
        "id": "230a5984"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timz815/360-NLP-Project/blob/main/finetune.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip3 install unsloth\n",
        "# pip3 install transformers datasets trl accelerate\n",
        "# pip3 install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "WuF4WpIn6MG9"
      },
      "id": "WuF4WpIn6MG9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "29bfd349",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "29bfd349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d122b3e-6a83-4f8b-e02c-cea379d49ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
            "  warnings.warn(f\"Failed to find {binary}\")\n",
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
            "  warnings.warn(f\"Failed to find {binary}\")\n",
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "W1130 14:31:02.860000 31432 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
            "[tensorflow|WARNING]From C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
            "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE_TORCH}:{i}\") for i in range(n_gpus)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.4: Fast Qwen3 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# ==========\n",
        "# load model\n",
        "# ==========\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c7975c70",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c7975c70"
      },
      "outputs": [],
      "source": [
        "# ============\n",
        "# load dataset\n",
        "# ============\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "jsonl_path = r\"C:\\Users\\timot\\Downloads\\nlp training\\movie_dialogue.jsonl\"\n",
        "data = [json.loads(line) for line in open(jsonl_path, encoding=\"utf-8\")]\n",
        "\n",
        "ds = Dataset.from_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# split data 80/10/10\n",
        "# ===================\n",
        "splits = ds.train_test_split(test_size=0.2, seed=42)\n",
        "test_ds = splits[\"test\"]\n",
        "train_val = splits[\"train\"]\n",
        "\n",
        "val_splits = train_val.train_test_split(test_size=0.125, seed=42)\n",
        "train_ds = val_splits[\"train\"]\n",
        "val_ds   = val_splits[\"test\"]\n",
        "\n",
        "print(\"Train:\", len(train_ds))\n",
        "print(\"Val:\",   len(val_ds))\n",
        "print(\"Test:\",  len(test_ds))\n",
        "\n",
        "raw_test = Dataset.from_list([\n",
        "    {\"english\": ex[\"english\"], \"chinese\": ex[\"chinese\"]}\n",
        "    for ex in test_ds\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "l_bd07wCacJ-",
        "outputId": "8f11b9c6-2eb4-482f-966e-16e126b492c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l_bd07wCacJ-",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 3679\n",
            "Val: 526\n",
            "Test: 1052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# format + token\n",
        "# ==============\n",
        "def format_example(ex, tokenizer):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": ex[\"chinese\"]},\n",
        "        {\"role\": \"assistant\", \"content\": ex[\"english\"]},\n",
        "        # for en to zh\n",
        "        #{\"role\": \"user\", \"content\": ex[\"english\"]},\n",
        "        #{\"role\": \"assistant\", \"content\": ex[\"chinese\"]},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "\n",
        "def tokenize_example(ex, tokenizer, max_length=2048):\n",
        "    tokens = tokenizer(\n",
        "        ex[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": tokens[\"input_ids\"],\n",
        "        \"attention_mask\": tokens[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "# apply to dataset\n",
        "train_ds = train_ds.map(\n",
        "    format_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=train_ds.column_names\n",
        ").map(\n",
        "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "val_ds = val_ds.map(\n",
        "    format_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=val_ds.column_names\n",
        ").map(\n",
        "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(\n",
        "    format_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=test_ds.column_names\n",
        ").map(\n",
        "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=[\"text\"]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtZQSVdx-v8k",
        "outputId": "0fba823f-9764-4ee1-82d0-680e3a0eaf38"
      },
      "id": "WtZQSVdx-v8k",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3679/3679 [00:00<00:00, 4849.40 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3679/3679 [00:00<00:00, 4761.46 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 526/526 [00:00<00:00, 3689.83 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 526/526 [00:00<00:00, 5911.76 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1052/1052 [00:00<00:00, 3722.38 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1052/1052 [00:00<00:00, 4825.72 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c7d9049f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c7d9049f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91b92ef-ae37-4912-e6a3-62c2ed36f778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.4 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# ====\n",
        "# LoRa\n",
        "# ====\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "63154389",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "63154389"
      },
      "outputs": [],
      "source": [
        "# ===============\n",
        "# training config\n",
        "# ===============\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch, datasets\n",
        "import os\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=20,\n",
        "    max_steps=600,\n",
        "    logging_steps=1,\n",
        "    output_dir=r\"C:\\Users\\timot\\Downloads\\nlp training\\qwen3-4b-dialogue-lora\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    completion_only_loss=False,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        ")\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "datasets.config.DEFAULT_MAX_BATCH_SIZE = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======\n",
        "# trainer\n",
        "# =======\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    args=sft_config,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hrSMe6siN6Un"
      },
      "id": "hrSMe6siN6Un",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unsloth config\n",
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.config.disable = True"
      ],
      "metadata": {
        "id": "wOT1GKOWEqRj"
      },
      "id": "wOT1GKOWEqRj",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====\n",
        "# train\n",
        "# =====\n",
        "trainer.train()\n",
        "\n",
        "# ===========================\n",
        "# save final loRa + tokenizer\n",
        "# ===========================\n",
        "save_dir = r\"C:\\Users\\timot\\Downloads\\nlp training\\qwen3-4b-dialogue-lora\\final\"\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eXW2NwGJOwW_",
        "outputId": "0294f2bc-c390-4be8-c613-dc7dbedeaa4c"
      },
      "id": "eXW2NwGJOwW_",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,679 | Num Epochs = 2 | Total steps = 600\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [600/600 40:50, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.966500</td>\n",
              "      <td>2.457101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.908300</td>\n",
              "      <td>1.834014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.823900</td>\n",
              "      <td>1.784690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.516500</td>\n",
              "      <td>1.742657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.744400</td>\n",
              "      <td>1.733152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.673400</td>\n",
              "      <td>1.709406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.624900</td>\n",
              "      <td>1.711472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.666500</td>\n",
              "      <td>1.687538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.140700</td>\n",
              "      <td>1.665807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.832500</td>\n",
              "      <td>1.659860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.697800</td>\n",
              "      <td>1.649849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.460000</td>\n",
              "      <td>1.639182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.612100</td>\n",
              "      <td>1.639126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.607300</td>\n",
              "      <td>1.627649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.616400</td>\n",
              "      <td>1.621247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.652600</td>\n",
              "      <td>1.615713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.222300</td>\n",
              "      <td>1.609293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.418900</td>\n",
              "      <td>1.608085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.996400</td>\n",
              "      <td>1.602098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.533300</td>\n",
              "      <td>1.595132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.623100</td>\n",
              "      <td>1.590694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.637300</td>\n",
              "      <td>1.583266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.672000</td>\n",
              "      <td>1.583396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.286200</td>\n",
              "      <td>1.582885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.453700</td>\n",
              "      <td>1.587430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.258200</td>\n",
              "      <td>1.590078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.279500</td>\n",
              "      <td>1.588983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.352600</td>\n",
              "      <td>1.591130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.320900</td>\n",
              "      <td>1.590987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.208900</td>\n",
              "      <td>1.590358</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\tokenizer_config.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\special_tokens_map.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\chat_template.jinja',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\vocab.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\merges.txt',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\added_tokens.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# save final loRa + tokenizer\n",
        "# ===========================\n",
        "save_dir = r\"C:\\Users\\timot\\Downloads\\nlp training\\qwen3-4b-dialogue-lora\\final\"\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk9PSRHr9gW8",
        "outputId": "441cb86e-e16a-48ad-dc02-9e3c79a3acb9"
      },
      "id": "Bk9PSRHr9gW8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\tokenizer_config.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\special_tokens_map.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\chat_template.jinja',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\vocab.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\merges.txt',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\added_tokens.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====\n",
        "# eval\n",
        "# ====\n",
        "#from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer\n",
        "#save_dir = r\"C:\\Users\\timot\\Downloads\\nlp training\\qwen3-4b-dialogue-lora\\final\"\n",
        "#model = AutoModelForCausalLM.from_pretrained(save_dir)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "\n",
        "metrics = trainer.evaluate(test_ds)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6kNs1u7NEqxL",
        "outputId": "19c3e171-0567-45b2-c2b0-73d46d3f4d2d"
      },
      "id": "6kNs1u7NEqxL",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='263' max='263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [263/263 00:41]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.5695916414260864, 'eval_runtime': 41.6516, 'eval_samples_per_second': 25.257, 'eval_steps_per_second': 6.314, 'epoch': 1.3043478260869565}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate sacrebleu\n"
      ],
      "metadata": {
        "id": "2din-OCF94Ps",
        "outputId": "95f024b4-abe9-405a-e8f2-d08efa6e30f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2din-OCF94Ps",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========\n",
        "# eval setup\n",
        "# ==========\n",
        "import evaluate\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from sacrebleu import sentence_bleu\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "FastLanguageModel.for_inference(model)\n"
      ],
      "metadata": {
        "id": "cEiqzzvi96oj",
        "outputId": "c14e776f-308e-4451-a52c-5a9d9df80606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cEiqzzvi96oj",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen3ForCausalLM(\n",
              "      (model): Qwen3Model(\n",
              "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (2): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (3): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (4): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (5): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (6): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (7-32): 26 x Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (33): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (34): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (35): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================\n",
        "# generation helper\n",
        "# =================\n",
        "\n",
        "def generate_english(chinese: str) -> str:\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": chinese}],  # â† Chinese input\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    gen = out[0][inputs.input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "# for en to zh\n",
        "# def generate_chinese(english: str) -> str:\n",
        "#     prompt = tokenizer.apply_chat_template(\n",
        "#         [{\"role\": \"user\", \"content\": english}],\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#     )\n",
        "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "#     with torch.no_grad():\n",
        "#         out = model.generate(\n",
        "#             **inputs,\n",
        "#             max_new_tokens=128,\n",
        "#             do_sample=False,  # greedy decoding\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "#     gen = out[0][inputs.input_ids.shape[-1]:]  # remove prompt tokens\n",
        "#     return tokenizer.decode(gen, skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "oc4tLty496vr"
      },
      "id": "oc4tLty496vr",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# run trained model on test set\n",
        "# =============================\n",
        "preds, refs = [], []\n",
        "for ex in tqdm(raw_test):\n",
        "    preds.append(generate_english(ex[\"chinese\"]))   # â† NEW NAME / CORRECT INPUT\n",
        "    refs.append(ex[\"english\"])\n",
        "\n",
        "# for en to zh\n",
        "# preds, refs = [], []\n",
        "# for ex in tqdm(raw_test):\n",
        "#     preds.append(generate_chinese(ex[\"english\"]))\n",
        "#     refs.append(ex[\"chinese\"])"
      ],
      "metadata": {
        "id": "YiL7QDAV962L",
        "outputId": "7b0417d1-928e-4f5c-8d44-e4717c819363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YiL7QDAV962L",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1052/1052 [30:09<00:00,  1.72s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q seqeval\n"
      ],
      "metadata": {
        "id": "NS9jgvF0A107",
        "outputId": "62260c10-1190-4dc0-857e-ee7548dab33d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NS9jgvF0A107",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# exact match + F1 eval\n",
        "# =====================\n",
        "import evaluate\n",
        "from collections import Counter\n",
        "\n",
        "# Exact-match\n",
        "exact_metric = evaluate.load(\"exact_match\")\n",
        "exact = exact_metric.compute(predictions=preds, references=refs)[\"exact_match\"]\n",
        "\n",
        "# Character-level F1 using Counter\n",
        "true_counter = Counter(''.join(refs))\n",
        "pred_counter = Counter(''.join(preds))\n",
        "\n",
        "tp = sum((true_counter & pred_counter).values())\n",
        "total_true = sum(true_counter.values())\n",
        "total_pred = sum(pred_counter.values())\n",
        "\n",
        "precision = tp / total_pred if total_pred else 0\n",
        "recall    = tp / total_true if total_true else 0\n",
        "char_f1   = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "\n",
        "# results\n",
        "print(f\"Exact-match accuracy : {exact:.2%}\")\n",
        "print(f\"Character-level F1  : {char_f1:.2%}\")\n"
      ],
      "metadata": {
        "id": "En6llfNW968T",
        "outputId": "5779ddae-b4d7-474f-f5e1-caefc495a0de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "En6llfNW968T",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact-match accuracy : 0.00%\n",
            "Character-level F1  : 73.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========\n",
        "# bleu 4 eval\n",
        "# ===========\n",
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "bleu = corpus_bleu(\n",
        "    preds,\n",
        "    [refs],\n",
        "    smooth_method=\"exp\"\n",
        ").score\n",
        "\n",
        "print(f\"BLEU-4 score : {bleu:.2f}\")"
      ],
      "metadata": {
        "id": "l0lj3LAw-llm",
        "outputId": "0ae996cc-e21c-4851-eaa8-78b52bd0a9ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l0lj3LAw-llm",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-4 score : 11.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1e3842",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "0d1e3842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55d49d2-340f-4674-d17b-8f9fb67649fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user\n",
            "Iâ€™m gonna make him an offer he canâ€™t refuse.\n",
            "assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "æˆ‘ gonna ç»™ä»–ä¸€ä¸ªä»–ä¹°ä¸èµ·çš„æ¡ä»¶\n"
          ]
        }
      ],
      "source": [
        "#QUICK TEST\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Iâ€™m gonna make him an offer he canâ€™t refuse.\"}]\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "out = model.generate(input_ids=inputs, max_new_tokens=128, temperature=0.7, do_sample=True)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zh_test_lines = {\n",
        "    \"sarcasm\": [\n",
        "        \"å“Žå‘€ï¼Œåˆæ˜¯å‘¨ä¸€æ—©ä¼šï¼ŒçœŸæ˜¯å¤ªâ€˜æƒŠå–œâ€™äº†ã€‚\",\n",
        "        \"æ‚¨å¯çœŸæ˜¯ä¸ªå¤©æ‰â€”â€”ä¸‹æ¬¡è®°å¾—çœ‹è¯´æ˜Žä¹¦å“¦ã€‚\",\n",
        "    ],\n",
        "    \"anger\": [\n",
        "        \"ç«‹åˆ»æ»šå‡ºæˆ‘çš„åŠžå…¬å®¤ï¼Œé©¬ä¸Šï¼\",\n",
        "        \"ä½ å†ç¢°ä¸€ä¸‹ï¼Œæˆ‘å‘èª“æ‰“æ–­ä½ çš„æ‰‹ã€‚\",\n",
        "    ],\n",
        "    \"ambiguous_tense_aspect\": [\n",
        "        \"æˆ‘åƒäº†è‹¹æžœã€‚\",          # ate / have eaten / had eaten?\n",
        "        \"å¥¹åœ¨å†™æŠ¥å‘Šã€‚\",          # is writing / was writing / will be writing?\n",
        "    ],\n",
        "    \"implicit_gender\": [\n",
        "        \"åŒ»ç”Ÿè¯´æˆ‘è¦ä¼‘æ¯ä¸€å‘¨ã€‚\",     # doctor gender unspecified\n",
        "        \"æŠ¤å£«æ¯å¤©æ—©ä¸Šç»™æˆ‘ç«¯èŒ¶ã€‚\",   # nurse gender unspecified\n",
        "        \"CEOæ˜¨å¤©å®£å¸ƒäº†æ–°æ”¿ã€‚\",      # CEO gender unspecified\n",
        "    ],\n",
        "    \"honorific_inflation\": [\n",
        "        \"è¯·ç»ç†ç­¾ä¸€ä¸‹ã€‚\",         # risk: â€œManager Sir, please signâ€\n",
        "        \"å‘Šè¯‰åŠ©ç†è®¢ä¸ªæˆ¿é—´ã€‚\",     # risk: â€œTell the assistant toâ€¦â€ â†’ â€œTell Assistant Zhangâ€\n",
        "    ],\n",
        "    \"chengyu_idiom\": [\n",
        "        \"ä»–éœ²å‡ºäº†é©¬è„šã€‚\",         # â€œshowed the horseâ€™s hoofâ€ â†’ spill the beans\n",
        "        \"é‚£æ®µæ¼”è®²çœŸæ˜¯å‚¬äººæ³ªä¸‹ã€‚\", # â€œmakes people shed tearsâ€ â†’ tear-jerker\n",
        "        \"å¥¹è„‘å­æœ‰ç‚¹ä¸æ­£å¸¸ã€‚\",     # literal: brain not normal â†’ â€œhas a screw looseâ€\n",
        "    ],\n",
        "    \"cultural_reference\": [\n",
        "        \"æˆ‘ä»¬ä¸åœ¨å ªè¨æ–¯äº†ã€‚\",     # direct loan of English idiom\n",
        "        \"ä»–åœ¨é‚£åœºè¾©è®ºä¸­é­é‡äº†æ»‘é“å¢ã€‚\",  # â€œmet his Waterlooâ€\n",
        "        \"å¬èµ·æ¥åƒã€Šé»‘é•œã€‹é‡Œçš„å‰§æƒ…ã€‚\",   # Black Mirror reference\n",
        "    ],\n",
        "    \"harmful_direct_speech\": [\n",
        "        \"ä½ è¿™ä¸ªè ¢è´§ï¼Œæˆ‘è¯´è¿‡åˆ«ç¢°å®ƒï¼\",\n",
        "        \"é—­å˜´ï¼Œæ­»åŸºä½¬ï¼Œæ²¡äººé—®ä½ ï¼\",\n",
        "    ],\n",
        "    \"classifier_mismatch\": [\n",
        "        \"ä¸€è¾†è½¦\",   # measure word è¾† â†’ risk: â€œa carâ€ vs â€œone carâ€\n",
        "        \"ä¸€æ¡ç‹—\",   # æ¡ â†’ â€œa dogâ€ (not â€œone strip of dogâ€)\n",
        "    ],\n",
        "    \"zero_pronoun\": [\n",
        "        \"çœ‹è§äº†ï¼Œå¾ˆå–œæ¬¢ï¼Œå°±ä¹°äº†ã€‚\",  # no explicit subject â†’ â€œI saw it, liked it, and bought it.â€\n",
        "    ],\n",
        "    \"relationship_explicitness\": [\n",
        "        \"æˆ‘å“¥å“¥æ˜¨å¤©ç»“å©šã€‚\",  # å“¥å“¥ â†’ older brother (English just â€œbrotherâ€)\n",
        "        \"å¥¹å§å§åœ¨é“¶è¡Œå·¥ä½œã€‚\", # å§å§ â†’ older sister\n",
        "    ],\n",
        "}\n",
        "\n",
        "for category, lines in zh_test_lines.items():\n",
        "    print(f\"--- ZHâ†’EN  {category} ---\\n\")\n",
        "    for zh in lines:\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": zh}],\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        out = model.generate(input_ids=inputs, max_new_tokens=128, temperature=0.7, do_sample=True)\n",
        "        en = tokenizer.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "        print(\"ZH:\", zh)\n",
        "        print(\"EN:\", en)\n",
        "        print()\n",
        "# ZH to EN\n",
        "# test_lines = {\n",
        "#     \"sarcasm\": [\n",
        "#         \"Oh great, another Monday morning meeting. Just what I needed.\",\n",
        "#         \"Yeah, youâ€™re a real geniusâ€”next time try reading the manual.\"\n",
        "#     ],\n",
        "#     \"anger\": [\n",
        "#         \"Get the hell out of my office. Now.\",\n",
        "#         \"I swear if you touch that again Iâ€™ll break your arm.\"\n",
        "#     ],\n",
        "#     \"ambiguous_tense\": [\n",
        "#         \"I saw the man with the telescope.\",\n",
        "#         \"The chicken is ready to eat.\"\n",
        "#     ],\n",
        "#     \"implicit_gender\": [\n",
        "#         \"The doctor said I should rest for a week.\",\n",
        "#         \"My nurse brought me tea every morning.\",\n",
        "#         \"The CEO announced a new policy yesterday.\"\n",
        "#     ],\n",
        "#     \"hierarchy_injection\": [\n",
        "#         \"Ask the manager to sign this.\",\n",
        "#         \"Tell the assistant to book a room.\",\n",
        "#     ],\n",
        "#     \"slang_idiom\": [\n",
        "#         \"He spilled the beans during the interrogation.\",\n",
        "#         \"That movie was a real tear-jerker.\",\n",
        "#         \"Sheâ€™s got a screw loose.\",\n",
        "#     ],\n",
        "#     \"cultural_reference\": [\n",
        "#         \"Weâ€™re not in Kansas anymore.\",\n",
        "#         \"He met his Waterloo in that debate.\",\n",
        "#         \"Sounds like something straight out of Black Mirror.\",\n",
        "#     ],\n",
        "#     \"harmful_direct_speech\": [\n",
        "#         \"You stupid bitch, I told you not to touch it!\",\n",
        "#         \"Shut up, faggot, nobody asked you.\",\n",
        "#     ],\n",
        "# }\n",
        "\n",
        "# # Run the test\n",
        "# for category, lines in test_lines.items():\n",
        "#     print(f\"--- Testing category: {category} ---\\n\")\n",
        "#     for line in lines:\n",
        "#         inputs = tokenizer.apply_chat_template(\n",
        "#             [{\"role\": \"user\", \"content\": line}],\n",
        "#             tokenize=True,\n",
        "#             add_generation_prompt=True,\n",
        "#             return_tensors=\"pt\"\n",
        "#         ).to(\"cuda\")\n",
        "\n",
        "#         out = model.generate(input_ids=inputs, max_new_tokens=128, temperature=0.7, do_sample=True)\n",
        "#         print(\"Input:\", line)\n",
        "#         print(\"Output:\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
        "#         print()\n"
      ],
      "metadata": {
        "id": "U2j7v3aNK9LM",
        "outputId": "60af2eb5-fd07-4495-9e90-faff804b50c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "U2j7v3aNK9LM",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ZHâ†’EN  sarcasm ---\n",
            "\n",
            "ZH: å“Žå‘€ï¼Œåˆæ˜¯å‘¨ä¸€æ—©ä¼šï¼ŒçœŸæ˜¯å¤ªâ€˜æƒŠå–œâ€™äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "It's always a shock to see this Monday morning meeting.\n",
            "\n",
            "ZH: æ‚¨å¯çœŸæ˜¯ä¸ªå¤©æ‰â€”â€”ä¸‹æ¬¡è®°å¾—çœ‹è¯´æ˜Žä¹¦å“¦ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Next time, please read the manual.\n",
            "\n",
            "--- ZHâ†’EN  anger ---\n",
            "\n",
            "ZH: ç«‹åˆ»æ»šå‡ºæˆ‘çš„åŠžå…¬å®¤ï¼Œé©¬ä¸Šï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Get out of my office right fucking now.\n",
            "\n",
            "ZH: ä½ å†ç¢°ä¸€ä¸‹ï¼Œæˆ‘å‘èª“æ‰“æ–­ä½ çš„æ‰‹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I'll cut off your fucking fingers, you understand?\n",
            "\n",
            "--- ZHâ†’EN  ambiguous_tense_aspect ---\n",
            "\n",
            "ZH: æˆ‘åƒäº†è‹¹æžœã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I ate an apple.\n",
            "\n",
            "ZH: å¥¹åœ¨å†™æŠ¥å‘Šã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "She's writing a report.\n",
            "\n",
            "--- ZHâ†’EN  implicit_gender ---\n",
            "\n",
            "ZH: åŒ»ç”Ÿè¯´æˆ‘è¦ä¼‘æ¯ä¸€å‘¨ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The doctor said I need to rest for a week.\n",
            "\n",
            "ZH: æŠ¤å£«æ¯å¤©æ—©ä¸Šç»™æˆ‘ç«¯èŒ¶ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "and they brought me tea every morning.\n",
            "\n",
            "ZH: CEOæ˜¨å¤©å®£å¸ƒäº†æ–°æ”¿ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "CEO announced new policies yesterday.\n",
            "\n",
            "--- ZHâ†’EN  honorific_inflation ---\n",
            "\n",
            "ZH: è¯·ç»ç†ç­¾ä¸€ä¸‹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Let me get your manager to sign it.\n",
            "\n",
            "ZH: å‘Šè¯‰åŠ©ç†è®¢ä¸ªæˆ¿é—´ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "and get an assignment for a room.\n",
            "\n",
            "--- ZHâ†’EN  chengyu_idiom ---\n",
            "\n",
            "ZH: ä»–éœ²å‡ºäº†é©¬è„šã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "He's been caught.\n",
            "\n",
            "ZH: é‚£æ®µæ¼”è®²çœŸæ˜¯å‚¬äººæ³ªä¸‹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "is one that brought tears to my eyes.\n",
            "\n",
            "ZH: å¥¹è„‘å­æœ‰ç‚¹ä¸æ­£å¸¸ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "She's got a little bit of a crazy brain.\n",
            "\n",
            "--- ZHâ†’EN  cultural_reference ---\n",
            "\n",
            "ZH: æˆ‘ä»¬ä¸åœ¨å ªè¨æ–¯äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "We're not in Kansas anymore.\n",
            "\n",
            "ZH: ä»–åœ¨é‚£åœºè¾©è®ºä¸­é­é‡äº†æ»‘é“å¢ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "he had a big fall on the way to the finish line.\n",
            "\n",
            "ZH: å¬èµ·æ¥åƒã€Šé»‘é•œã€‹é‡Œçš„å‰§æƒ…ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "It sounds like something from Black Mirror.\n",
            "\n",
            "--- ZHâ†’EN  harmful_direct_speech ---\n",
            "\n",
            "ZH: ä½ è¿™ä¸ªè ¢è´§ï¼Œæˆ‘è¯´è¿‡åˆ«ç¢°å®ƒï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I told you not to touch it, fuckface!\n",
            "\n",
            "ZH: é—­å˜´ï¼Œæ­»åŸºä½¬ï¼Œæ²¡äººé—®ä½ ï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Shut the fuck up, fuckin' dead man. Nobody asked you!\n",
            "\n",
            "--- ZHâ†’EN  classifier_mismatch ---\n",
            "\n",
            "ZH: ä¸€è¾†è½¦\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "One car.\n",
            "\n",
            "ZH: ä¸€æ¡ç‹—\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "A dog.\n",
            "\n",
            "--- ZHâ†’EN  zero_pronoun ---\n",
            "\n",
            "ZH: çœ‹è§äº†ï¼Œå¾ˆå–œæ¬¢ï¼Œå°±ä¹°äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Yeah, I saw it, and I liked it, and I bought it.\n",
            "\n",
            "--- ZHâ†’EN  relationship_explicitness ---\n",
            "\n",
            "ZH: æˆ‘å“¥å“¥æ˜¨å¤©ç»“å©šã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "My brother got married yesterday.\n",
            "\n",
            "ZH: å¥¹å§å§åœ¨é“¶è¡Œå·¥ä½œã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Her sister works at the bank.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(save_dir)"
      ],
      "metadata": {
        "id": "22qKOpNCZ2fW"
      },
      "id": "22qKOpNCZ2fW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c251732b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c251732b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54092408-c2c9-45a6-ae07-1dad8e8349b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel, tokenizer = FastLanguageModel.from_pretrained(\\n    \"my-lora-adapter\",\\n    load_in_4bit=True,\\n)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# ===========================\n",
        "# save hf format LoRa adapter\n",
        "# ===========================\n",
        "model.save_pretrained(\"my-lora-adapter\")   # HF format\n",
        "tokenizer.save_pretrained(\"my-lora-adapter\")\n",
        "\n",
        "# load:\n",
        "\"\"\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"my-lora-adapter\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}