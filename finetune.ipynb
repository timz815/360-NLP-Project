{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "230a5984",
      "metadata": {
        "id": "230a5984"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timz815/360-NLP-Project/blob/main/finetune.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip3 install unsloth\n",
        "# pip3 install transformers datasets trl accelerate\n",
        "# pip3 install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "WuF4WpIn6MG9"
      },
      "id": "WuF4WpIn6MG9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "29bfd349",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "29bfd349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb72517a-11f2-4b17-fc9b-e3da5e61767a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
            "  warnings.warn(f\"Failed to find {binary}\")\n",
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
            "  warnings.warn(f\"Failed to find {binary}\")\n",
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "W1205 20:34:37.774000 28372 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
            "[tensorflow|WARNING]From C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
            "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE_TORCH}:{i}\") for i in range(n_gpus)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.4: Fast Qwen3 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# ==========\n",
        "# load model\n",
        "# ==========\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c7975c70",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c7975c70"
      },
      "outputs": [],
      "source": [
        "# ============\n",
        "# load dataset\n",
        "# ============\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "jsonl_path = r\"C:\\Users\\timot\\Downloads\\nlp training\\movie_dialogue.jsonl\"\n",
        "data = [json.loads(line) for line in open(jsonl_path, encoding=\"utf-8\")]\n",
        "\n",
        "ds = Dataset.from_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# split data 85/10/5\n",
        "# ===================\n",
        "# 10% test takes too long\n",
        "splits = ds.train_test_split(test_size=0.05, seed=42)\n",
        "test_ds = splits[\"test\"]\n",
        "train_val = splits[\"train\"]\n",
        "\n",
        "val_fraction = 0.10 / 0.95\n",
        "val_splits = train_val.train_test_split(test_size=val_fraction, seed=42)\n",
        "train_ds = val_splits[\"train\"]\n",
        "val_ds   = val_splits[\"test\"]\n",
        "\n",
        "print(\"Train:\", len(train_ds))\n",
        "print(\"Val:\",   len(val_ds))\n",
        "print(\"Test:\",  len(test_ds))\n",
        "\n",
        "raw_test = Dataset.from_list([\n",
        "    {\"english\": ex[\"english\"], \"chinese\": ex[\"chinese\"]}\n",
        "    for ex in test_ds\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "l_bd07wCacJ-",
        "outputId": "513496fb-4c02-4ce4-ae6e-1bd75d4d8996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l_bd07wCacJ-",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 8023\n",
            "Val: 945\n",
            "Test: 473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# format + token\n",
        "# ==============\n",
        "def format_example(ex, tokenizer):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": ex[\"chinese\"]},\n",
        "        {\"role\": \"assistant\", \"content\": ex[\"english\"]},\n",
        "        # for en to zh\n",
        "        #{\"role\": \"user\", \"content\": ex[\"english\"]},\n",
        "        #{\"role\": \"assistant\", \"content\": ex[\"chinese\"]},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "\n",
        "def tokenize_example(ex, tokenizer, max_length=2048):\n",
        "    tokens = tokenizer(\n",
        "        ex[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": tokens[\"input_ids\"],\n",
        "        \"attention_mask\": tokens[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "# apply to dataset\n",
        "train_ds = train_ds.map(\n",
        "    format_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=train_ds.column_names\n",
        ").map(\n",
        "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "val_ds = val_ds.map(\n",
        "    format_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=val_ds.column_names\n",
        ").map(\n",
        "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(\n",
        "    format_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=test_ds.column_names\n",
        ").map(\n",
        "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    remove_columns=[\"text\"]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtZQSVdx-v8k",
        "outputId": "03948f5c-53c1-4cd7-955e-4bd16f6239f1"
      },
      "id": "WtZQSVdx-v8k",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8023/8023 [00:01<00:00, 4380.34 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8023/8023 [00:01<00:00, 4607.96 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 945/945 [00:00<00:00, 2902.97 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 945/945 [00:00<00:00, 4241.59 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:00<00:00, 3224.40 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:00<00:00, 3307.02 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c7d9049f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c7d9049f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d3d539-5469-4016-a312-d2733efe3fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.4 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# ====\n",
        "# LoRa\n",
        "# ====\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "63154389",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "63154389"
      },
      "outputs": [],
      "source": [
        "# ===============\n",
        "# training config\n",
        "# ===============\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch, datasets\n",
        "import os\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=20,\n",
        "    max_steps=400,\n",
        "    logging_steps=1,\n",
        "    output_dir=r\"C:\\Users\\timot\\Downloads\\nlp training\\qwen3-4b-dialogue-lora\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=3.7e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    completion_only_loss=False,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        ")\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "datasets.config.DEFAULT_MAX_BATCH_SIZE = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======\n",
        "# trainer\n",
        "# =======\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    args=sft_config,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hrSMe6siN6Un"
      },
      "id": "hrSMe6siN6Un",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unsloth config\n",
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.config.disable = True"
      ],
      "metadata": {
        "id": "wOT1GKOWEqRj"
      },
      "id": "wOT1GKOWEqRj",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====\n",
        "# train\n",
        "# =====\n",
        "trainer.train()\n",
        "\n",
        "# ===========================\n",
        "# save final loRa + tokenizer\n",
        "# ===========================\n",
        "save_dir = r\"C:\\Users\\timot\\Downloads\\nlp training\\qwen3-4b-dialogue-lora\\final\"\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "id": "eXW2NwGJOwW_",
        "outputId": "0d371feb-5097-4abc-bfbc-73f2ddebe249"
      },
      "id": "eXW2NwGJOwW_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 8,023 | Num Epochs = 1 | Total steps = 400\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 33:58, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.272100</td>\n",
              "      <td>2.309618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.562400</td>\n",
              "      <td>1.730411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.504000</td>\n",
              "      <td>1.686203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.739000</td>\n",
              "      <td>1.657347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.409400</td>\n",
              "      <td>1.641471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.719400</td>\n",
              "      <td>1.639650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.707900</td>\n",
              "      <td>1.613685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.847500</td>\n",
              "      <td>1.589851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.760100</td>\n",
              "      <td>1.597790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.634400</td>\n",
              "      <td>1.572113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.649000</td>\n",
              "      <td>1.575659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.719500</td>\n",
              "      <td>1.560945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.540100</td>\n",
              "      <td>1.553416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.303100</td>\n",
              "      <td>1.545863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.569100</td>\n",
              "      <td>1.542502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.499400</td>\n",
              "      <td>1.537898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.208300</td>\n",
              "      <td>1.531839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.680600</td>\n",
              "      <td>1.524210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.548500</td>\n",
              "      <td>1.522345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.706100</td>\n",
              "      <td>1.521648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\tokenizer_config.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\special_tokens_map.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\chat_template.jinja',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\vocab.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\merges.txt',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\added_tokens.json',\n",
              " 'C:\\\\Users\\\\timot\\\\Downloads\\\\nlp training\\\\qwen3-4b-dialogue-lora\\\\final\\\\tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====\n",
        "# eval\n",
        "# ====\n",
        "metrics = trainer.evaluate(test_ds)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6kNs1u7NEqxL",
        "outputId": "308e65b5-0d9e-4b8a-c493-98ee1a1fdb87"
      },
      "id": "6kNs1u7NEqxL",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [119/119 00:20]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.4759572744369507, 'eval_runtime': 21.234, 'eval_samples_per_second': 22.276, 'eval_steps_per_second': 5.604, 'epoch': 0.3988035892323031}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========\n",
        "# eval setup\n",
        "# ==========\n",
        "!pip install -q evaluate seqeval sentence-transformers\n",
        "import evaluate\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "FastLanguageModel.for_inference(model)\n"
      ],
      "metadata": {
        "id": "cEiqzzvi96oj",
        "outputId": "8956bb6a-b4df-41c2-adc9-5a12ccbb8a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cEiqzzvi96oj",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen3ForCausalLM(\n",
              "      (model): Qwen3Model(\n",
              "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (2): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (3): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (4): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (5): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (6): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (7-32): 26 x Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (33): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (34): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "          (35): Qwen3DecoderLayer(\n",
              "            (self_attn): Qwen3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9728, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9728, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================\n",
        "# generation helper\n",
        "# =================\n",
        "\n",
        "# def generate_english(chinese: str) -> str:\n",
        "#     prompt = tokenizer.apply_chat_template(\n",
        "#         [{\"role\": \"user\", \"content\": chinese}],  # â† Chinese input\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#     )\n",
        "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "#     with torch.no_grad():\n",
        "#         out = model.generate(\n",
        "#             **inputs,\n",
        "#             max_new_tokens=128,\n",
        "#             do_sample=False,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "#     gen = out[0][inputs.input_ids.shape[-1]:]\n",
        "#     return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "def generate_english(chinese: str) -> str:\n",
        "    system_prompt = (\n",
        "        \"You are a faithful Chineseâ†’English translator. \"\n",
        "        \"Follow these rules:\\n\"\n",
        "        \"1. Do not infer gender unless the source explicitly states it.\\n\"\n",
        "        \"2. Translate faithfully; do not add new insults or escalate rudeness.\\n\"\n",
        "        \"3. Do not reinterpret sensitive termsâ€”keep original nuance.\\n\"\n",
        "        \"4. Output only the English translation, no explanations.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": chinese}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    gen = out[0][inputs.input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "oc4tLty496vr"
      },
      "id": "oc4tLty496vr",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# run trained model on test set\n",
        "# =============================\n",
        "preds, refs = [], []\n",
        "for ex in tqdm(raw_test):\n",
        "    preds.append(generate_english(ex[\"chinese\"]))\n",
        "    refs.append(ex[\"english\"])\n",
        "\n",
        "# for en to zh\n",
        "# preds, refs = [], []\n",
        "# for ex in tqdm(raw_test):\n",
        "#     preds.append(generate_chinese(ex[\"english\"]))\n",
        "#     refs.append(ex[\"chinese\"])"
      ],
      "metadata": {
        "id": "YiL7QDAV962L",
        "outputId": "b46013b1-d4d5-4380-95ef-a7799ef1aee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YiL7QDAV962L",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [14:54<00:00,  1.89s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================\n",
        "# f1 and semantic similarity report\n",
        "# =================================\n",
        "from collections import Counter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# ------- character-level F1 (bag of words similarity) -------\n",
        "def char_f1(preds, refs):\n",
        "    true_counter = Counter(''.join(refs))\n",
        "    pred_counter = Counter(''.join(preds))\n",
        "    tp = sum((true_counter & pred_counter).values())\n",
        "    prec = tp / max(sum(pred_counter.values()), 1e-9)\n",
        "    rec  = tp / max(sum(true_counter.values()), 1e-9)\n",
        "    return 2 * prec * rec / max(prec + rec, 1e-9)\n",
        "\n",
        "# ---------- semantic similarity ----------\n",
        "model_sim = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "emb_pred = model_sim.encode(preds, convert_to_tensor=True, normalize_embeddings=True)\n",
        "emb_ref  = model_sim.encode(refs,  convert_to_tensor=True, normalize_embeddings=True)\n",
        "sbert_scores = util.pytorch_cos_sim(emb_pred, emb_ref).diag().cpu().tolist()\n",
        "\n",
        "print(f\"Character-level F1 : {char_f1(preds, refs):.2%}\")\n",
        "print(f\"SBERT similarity   : {sum(sbert_scores)/len(sbert_scores):.2%}\")"
      ],
      "metadata": {
        "id": "NS9jgvF0A107",
        "outputId": "8cf75cda-cd20-4e92-d173-56a7e8cf4594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NS9jgvF0A107",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character-level F1 : 72.31%\n",
            "SBERT similarity   : 68.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zh_test_lines = {\n",
        "    \"sarcasm\": [\n",
        "        \"å“Žå‘€ï¼Œåˆæ˜¯å‘¨ä¸€æ—©ä¼šï¼ŒçœŸæ˜¯å¤ªâ€˜æƒŠå–œâ€™äº†ã€‚\",\n",
        "        \"æ‚¨å¯çœŸæ˜¯ä¸ªå¤©æ‰â€”â€”ä¸‹æ¬¡è®°å¾—çœ‹è¯´æ˜Žä¹¦å“¦ã€‚\",\n",
        "        \"å“‡ï¼Œè¿™ç½‘é€ŸçœŸå¿«ï¼Œæˆ‘éƒ½å¿«ç¡ç€äº†ã€‚\",\n",
        "        \"æ‚¨è¿™â€˜é«˜æ•ˆçŽ‡â€™çœŸè®©æˆ‘å¤§å¼€çœ¼ç•Œã€‚\",\n",
        "        \"å¤ªå¥½äº†ï¼ŒåˆåŠ ç­åˆ°åç‚¹ï¼Œäººç”Ÿå·…å³°å•Šã€‚\",\n",
        "    ],\n",
        "    \"anger\": [\n",
        "        \"ç«‹åˆ»æ»šå‡ºæˆ‘çš„åŠžå…¬å®¤ï¼Œé©¬ä¸Šï¼\",\n",
        "        \"ä½ å†ç¢°ä¸€ä¸‹ï¼Œæˆ‘å‘èª“æ‰“æ–­ä½ çš„æ‰‹ã€‚\",\n",
        "        \"åˆ«å†è®©æˆ‘çœ‹åˆ°ä½ ï¼Œæ»šï¼\",\n",
        "        \"ä½ è„‘å­è¿›æ°´äº†å—ï¼Ÿ\",\n",
        "        \"ç«‹åˆ»ç»™æˆ‘æ¶ˆå¤±ï¼Œå¦åˆ™åŽæžœè‡ªè´Ÿï¼\",\n",
        "    ],\n",
        "    \"ambiguous_tense_aspect\": [\n",
        "        \"æˆ‘åƒäº†è‹¹æžœã€‚\",          # ate / have eaten / had eaten?\n",
        "        \"å¥¹åœ¨å†™æŠ¥å‘Šã€‚\",          # is writing / was writing / will be writing?\n",
        "        \"æˆ‘å†™äº†ä¿¡ã€‚\",          # wrote / have written / had written?\n",
        "        \"ä»–ä»¬æ­£åœ¨åƒé¥­ã€‚\",      # are eating / were eating / will be eating?\n",
        "        \"ä»–èµ°äº†ã€‚\",            # left / has left / had left?\n",
        "    ],\n",
        "    \"implicit_gender\": [\n",
        "        \"åŒ»ç”Ÿè¯´æˆ‘è¦ä¼‘æ¯ä¸€å‘¨ã€‚\",     # doctor gender unspecified\n",
        "        \"æŠ¤å£«æ¯å¤©æ—©ä¸Šç»™æˆ‘ç«¯èŒ¶ã€‚\",   # nurse gender unspecified\n",
        "        \"CEOæ˜¨å¤©å®£å¸ƒäº†æ–°æ”¿ã€‚\",      # CEO gender unspecified\n",
        "        \"ç§˜ä¹¦è¯´ä¼šè®®æ”¹åˆ°ä¸‰ç‚¹ã€‚\",      # secretary gender unspecified\n",
        "        \"å¸æœºåœ¨æ¥¼ä¸‹ç­‰ã€‚\",            # driver gender unspecified\n",
        "        \"å¾‹å¸ˆé€šçŸ¥æˆ‘ä»¬èƒœè¯‰äº†ã€‚\",      # lawyer gender unspecified\n",
        "    ],\n",
        "    \"honorific_inflation\": [\n",
        "        \"è¯·ç»ç†ç­¾ä¸€ä¸‹ã€‚\",         #  â€œManager, please signâ€\n",
        "        \"å‘Šè¯‰åŠ©ç†è®¢ä¸ªæˆ¿é—´ã€‚\",     #  â€œTell the assistant toâ€¦â€ â†’ â€œTell Assistant Zhangâ€\n",
        "        \"è¯·æ€»ç›‘è¿‡ç›®ã€‚\",          #  â€œDirector, please lookâ€\n",
        "        \"è®©ç§˜ä¹¦è”ç³»å®¢æˆ·ã€‚\",      # â€œSecretary Zhang will contactâ€\n",
        "        \"å·²å‘ç»™å‰¯æ€»å®¡æ‰¹ã€‚\",      # â€œVice-President approvedâ€\n",
        "    ],\n",
        "    \"chengyu_idiom\": [\n",
        "        \"ä»–éœ²å‡ºäº†é©¬è„šã€‚\",         # â€œshowed the horseâ€™s hoofâ€ â†’ spill the beans\n",
        "        \"é‚£æ®µæ¼”è®²çœŸæ˜¯å‚¬äººæ³ªä¸‹ã€‚\", # â€œmakes people shed tearsâ€ â†’ tear-jerker\n",
        "        \"å¥¹è„‘å­æœ‰ç‚¹ä¸æ­£å¸¸ã€‚\",     # literal: brain not normal â†’ â€œhas a screw looseâ€\n",
        "        \"ä»–ç®€ç›´æžäººå¿§å¤©ã€‚\",       # â€œlike the man of Qi who feared the sky would fallâ€ â†’ needless worry\n",
        "        \"åˆ«ç­é—¨å¼„æ–§äº†ã€‚\",         # â€œshow off your axe in front of Lu Banâ€ â†’ teach a fish to swim\n",
        "        \"è¿™äº‹å·²ç»æœ¨å·²æˆèˆŸã€‚\",      # â€œthe wood is already a boatâ€ â†’ whatâ€™s done is done\n",
        "    ],\n",
        "    \"cultural_reference\": [\n",
        "        \"æˆ‘ä»¬ä¸åœ¨å ªè¨æ–¯äº†ã€‚\",     # direct loan of English idiom\n",
        "        \"ä»–åœ¨é‚£åœºè¾©è®ºä¸­é­é‡äº†æ»‘é“å¢ã€‚\",  # â€œmet his Waterlooâ€\n",
        "        \"å¬èµ·æ¥åƒã€Šé»‘é•œã€‹é‡Œçš„å‰§æƒ…ã€‚\",   # Black Mirror reference\n",
        "        \"ä»–å°±åƒå ‚å‰è¯ƒå¾·ä¸€æ ·å†²å‘é£Žè½¦ã€‚\",  # Don Quixote reference\n",
        "        \"è¿™æ“ä½œå ªæ¯”ã€Šç”„å¬›ä¼ ã€‹çš„å®«æ–—ã€‚\",  # Empresses in the Palace reference\n",
        "        \"ç®€ç›´æ˜¯çŽ°å®žç‰ˆã€Šæ¥šé—¨çš„ä¸–ç•Œã€‹ã€‚\",   # Truman Show reference\n",
        "    ],\n",
        "    \"harmful_direct_speech\": [\n",
        "        \"ä½ è¿™ä¸ªè ¢è´§ï¼Œæˆ‘è¯´è¿‡åˆ«ç¢°å®ƒï¼\",\n",
        "        \"é—­å˜´ï¼Œæ­»åŸºä½¬ï¼Œæ²¡äººé—®ä½ ï¼\",\n",
        "        \"åºŸç‰©ï¼Œè¿žè¿™ç‚¹äº‹éƒ½åšä¸å¥½ï¼\",\n",
        "        \"æ»šå¼€ï¼Œæ­»èƒ–å­ï¼ŒæŒ¡ä½è·¯äº†ï¼\",\n",
        "        \"è‡­å¥³äººï¼Œå°‘åœ¨è¿™é‡Œè£…æ¸…é«˜ï¼\",\n",
        "    ],\n",
        "    \"classifier_mismatch\": [\n",
        "        \"ä¸€è¾†è½¦\",   # measure word è¾† â†’ risk: â€œa carâ€ vs â€œone carâ€\n",
        "        \"ä¸€æ¡ç‹—\",   # æ¡ â†’ â€œa dogâ€ (not â€œone strip of dogâ€)\n",
        "        \"ä¸€å¼ æ¡Œå­\",   # å¼  â†’ â€œa tableâ€ (not â€œone sheet of tableâ€)\n",
        "        \"ä¸€åªçŒ«\",     # åª â†’ â€œa catâ€ (not â€œone hand of catâ€)\n",
        "        \"ä¸€é¢—æ˜Ÿæ˜Ÿ\",   # é¢— â†’ â€œa starâ€ (not â€œone grain of starâ€)\n",
        "    ],\n",
        "    \"zero_pronoun\": [\n",
        "        \"çœ‹è§äº†ï¼Œå¾ˆå–œæ¬¢ï¼Œå°±ä¹°äº†ã€‚\",  # no explicit subject â†’ â€œ[I] saw it, liked it, and bought it.â€\n",
        "        \"æ‰“å¼€ç”µè§†ï¼Œå‘çŽ°æ’­å¹¿å‘Šï¼Œå°±å…³äº†ã€‚\",\n",
        "        \"æ”¶åˆ°é‚®ä»¶ï¼Œè¯»å®Œï¼Œç›´æŽ¥åˆ é™¤ã€‚\",\n",
        "        \"è·¯è¿‡ä¹¦åº—ï¼Œè¿›åŽ»é€›äº†åŠå°æ—¶ã€‚\",\n",
        "    ],\n",
        "    \"relationship_explicitness\": [\n",
        "        \"æˆ‘å“¥å“¥æ˜¨å¤©ç»“å©šã€‚\",  # å“¥å“¥ â†’ older brother\n",
        "        \"å¥¹å§å§åœ¨é“¶è¡Œå·¥ä½œã€‚\", # å§å§ â†’ older sister\n",
        "        \"æˆ‘å ‚å“¥åœ¨å›½å¤–å®šå±…ã€‚\",    # å ‚å“¥ â†’ older male cousin (fatherâ€™s side)\n",
        "    ],\n",
        "}\n",
        "\n",
        "for category, lines in zh_test_lines.items():\n",
        "    print(f\"--- ZHâ†’EN  {category} ---\\n\")\n",
        "    for zh in lines:\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": zh}],\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        out = model.generate(input_ids=inputs, max_new_tokens=128, temperature=0.7, do_sample=True)\n",
        "        en = tokenizer.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "        print(\"ZH:\", zh)\n",
        "        print(\"EN:\", en)\n",
        "        print()"
      ],
      "metadata": {
        "id": "U2j7v3aNK9LM",
        "outputId": "012a0fd9-e107-4df7-e57e-d2fca41b1d24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "U2j7v3aNK9LM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ZHâ†’EN  sarcasm ---\n",
            "\n",
            "ZH: å“Žå‘€ï¼Œåˆæ˜¯å‘¨ä¸€æ—©ä¼šï¼ŒçœŸæ˜¯å¤ªâ€˜æƒŠå–œâ€™äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Oh, it's a Monday morning meeting, too.\n",
            "\n",
            "ZH: æ‚¨å¯çœŸæ˜¯ä¸ªå¤©æ‰â€”â€”ä¸‹æ¬¡è®°å¾—çœ‹è¯´æ˜Žä¹¦å“¦ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Youâ€™re a genius, though.  Please, next time, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please, please\n",
            "\n",
            "ZH: å“‡ï¼Œè¿™ç½‘é€ŸçœŸå¿«ï¼Œæˆ‘éƒ½å¿«ç¡ç€äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Wow, thatâ€™s a fast connection. Iâ€™m so sleepy.\n",
            "\n",
            "ZH: æ‚¨è¿™â€˜é«˜æ•ˆçŽ‡â€™çœŸè®©æˆ‘å¤§å¼€çœ¼ç•Œã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "and Iâ€™m impressed with the efficiency.\n",
            "\n",
            "ZH: å¤ªå¥½äº†ï¼ŒåˆåŠ ç­åˆ°åç‚¹ï¼Œäººç”Ÿå·…å³°å•Šã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Thatâ€™s good, working overtime till ten, man.\n",
            "\n",
            "--- ZHâ†’EN  anger ---\n",
            "\n",
            "ZH: ç«‹åˆ»æ»šå‡ºæˆ‘çš„åŠžå…¬å®¤ï¼Œé©¬ä¸Šï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Get out of my office right now!\n",
            "\n",
            "ZH: ä½ å†ç¢°ä¸€ä¸‹ï¼Œæˆ‘å‘èª“æ‰“æ–­ä½ çš„æ‰‹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I swear I'll break your fucking fingers if you touch me again.\n",
            "\n",
            "ZH: åˆ«å†è®©æˆ‘çœ‹åˆ°ä½ ï¼Œæ»šï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Donâ€™t make me see you again, fucker!\n",
            "\n",
            "ZH: ä½ è„‘å­è¿›æ°´äº†å—ï¼Ÿ\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "You lost your mind?\n",
            "\n",
            "ZH: ç«‹åˆ»ç»™æˆ‘æ¶ˆå¤±ï¼Œå¦åˆ™åŽæžœè‡ªè´Ÿï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Get the fuck out of here! Or you're gonna be the next one.\n",
            "\n",
            "--- ZHâ†’EN  ambiguous_tense_aspect ---\n",
            "\n",
            "ZH: æˆ‘åƒäº†è‹¹æžœã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I ate an apple.\n",
            "\n",
            "ZH: å¥¹åœ¨å†™æŠ¥å‘Šã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "She's writing a report.\n",
            "\n",
            "ZH: æˆ‘å†™äº†ä¿¡ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I wrote a letter.\n",
            "\n",
            "ZH: ä»–ä»¬æ­£åœ¨åƒé¥­ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Theyâ€™re having lunch.\n",
            "\n",
            "ZH: ä»–èµ°äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "He's gone.\n",
            "\n",
            "--- ZHâ†’EN  implicit_gender ---\n",
            "\n",
            "ZH: åŒ»ç”Ÿè¯´æˆ‘è¦ä¼‘æ¯ä¸€å‘¨ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "My doctor told me I need to rest for a week.\n",
            "\n",
            "ZH: æŠ¤å£«æ¯å¤©æ—©ä¸Šç»™æˆ‘ç«¯èŒ¶ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "My nurses brought me tea every morning.\n",
            "\n",
            "ZH: CEOæ˜¨å¤©å®£å¸ƒäº†æ–°æ”¿ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The CEO announced the new policies yesterday.\n",
            "\n",
            "ZH: ç§˜ä¹¦è¯´ä¼šè®®æ”¹åˆ°ä¸‰ç‚¹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The secretary said the meeting is moved to 1:00.\n",
            "\n",
            "ZH: å¸æœºåœ¨æ¥¼ä¸‹ç­‰ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The driver is waiting downstairs.\n",
            "\n",
            "ZH: å¾‹å¸ˆé€šçŸ¥æˆ‘ä»¬èƒœè¯‰äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The attorney informs us of a win.\n",
            "\n",
            "--- ZHâ†’EN  honorific_inflation ---\n",
            "\n",
            "ZH: è¯·ç»ç†ç­¾ä¸€ä¸‹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Sign it please, sir.\n",
            "\n",
            "ZH: å‘Šè¯‰åŠ©ç†è®¢ä¸ªæˆ¿é—´ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Tell the assistant to make a room.\n",
            "\n",
            "ZH: è¯·æ€»ç›‘è¿‡ç›®ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "For the directorâ€™s eyes.\n",
            "\n",
            "ZH: è®©ç§˜ä¹¦è”ç³»å®¢æˆ·ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Let the secretary call the client.\n",
            "\n",
            "ZH: å·²å‘ç»™å‰¯æ€»å®¡æ‰¹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "It's been sent to the vice president.\n",
            "\n",
            "--- ZHâ†’EN  chengyu_idiom ---\n",
            "\n",
            "ZH: ä»–éœ²å‡ºäº†é©¬è„šã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "He's exposed.\n",
            "\n",
            "ZH: é‚£æ®µæ¼”è®²çœŸæ˜¯å‚¬äººæ³ªä¸‹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "That speech really moved me.\n",
            "\n",
            "ZH: å¥¹è„‘å­æœ‰ç‚¹ä¸æ­£å¸¸ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Sheâ€™s a bit crazy.\n",
            "\n",
            "ZH: ä»–ç®€ç›´æžäººå¿§å¤©ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "He's just a worrywart.\n",
            "\n",
            "ZH: åˆ«ç­é—¨å¼„æ–§äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Don't act like you're an expert.\n",
            "\n",
            "ZH: è¿™äº‹å·²ç»æœ¨å·²æˆèˆŸã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "This is already a done deal.\n",
            "\n",
            "--- ZHâ†’EN  cultural_reference ---\n",
            "\n",
            "ZH: æˆ‘ä»¬ä¸åœ¨å ªè¨æ–¯äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "We ainâ€™t in Kansas anymore.\n",
            "\n",
            "ZH: ä»–åœ¨é‚£åœºè¾©è®ºä¸­é­é‡äº†æ»‘é“å¢ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "he had a lot of trouble in the debate.\n",
            "\n",
            "ZH: å¬èµ·æ¥åƒã€Šé»‘é•œã€‹é‡Œçš„å‰§æƒ…ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Sounds like something from Black Mirror.\n",
            "\n",
            "ZH: ä»–å°±åƒå ‚å‰è¯ƒå¾·ä¸€æ ·å†²å‘é£Žè½¦ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "He's like Don Quixote charging at a windmill.\n",
            "\n",
            "ZH: è¿™æ“ä½œå ªæ¯”ã€Šç”„å¬›ä¼ ã€‹çš„å®«æ–—ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "It's like a power struggle in \"Empresses in the Palace.\"\n",
            "\n",
            "ZH: ç®€ç›´æ˜¯çŽ°å®žç‰ˆã€Šæ¥šé—¨çš„ä¸–ç•Œã€‹ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Like a real-life version of The Truman Show.\n",
            "\n",
            "--- ZHâ†’EN  harmful_direct_speech ---\n",
            "\n",
            "ZH: ä½ è¿™ä¸ªè ¢è´§ï¼Œæˆ‘è¯´è¿‡åˆ«ç¢°å®ƒï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I told you not to touch it!\n",
            "\n",
            "ZH: é—­å˜´ï¼Œæ­»åŸºä½¬ï¼Œæ²¡äººé—®ä½ ï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Shut up, fucker! Nobody asked you!\n",
            "\n",
            "ZH: åºŸç‰©ï¼Œè¿žè¿™ç‚¹äº‹éƒ½åšä¸å¥½ï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Fuck, you're not even getting this!\n",
            "\n",
            "ZH: æ»šå¼€ï¼Œæ­»èƒ–å­ï¼ŒæŒ¡ä½è·¯äº†ï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Get the fuck out of the way, fuckin' fat fucker, blocking the fuckin' way!\n",
            "\n",
            "ZH: è‡­å¥³äººï¼Œå°‘åœ¨è¿™é‡Œè£…æ¸…é«˜ï¼\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Shut up, you motherfucking fuck!\n",
            "\n",
            "--- ZHâ†’EN  classifier_mismatch ---\n",
            "\n",
            "ZH: ä¸€è¾†è½¦\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "A car.\n",
            "\n",
            "ZH: ä¸€æ¡ç‹—\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "A dog.\n",
            "\n",
            "ZH: ä¸€å¼ æ¡Œå­\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "A table.\n",
            "\n",
            "ZH: ä¸€åªçŒ«\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "A cat.\n",
            "\n",
            "ZH: ä¸€é¢—æ˜Ÿæ˜Ÿ\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "A star.\n",
            "\n",
            "--- ZHâ†’EN  zero_pronoun ---\n",
            "\n",
            "ZH: çœ‹è§äº†ï¼Œå¾ˆå–œæ¬¢ï¼Œå°±ä¹°äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I see, I like it, I bought it.\n",
            "\n",
            "ZH: æ‰“å¼€ç”µè§†ï¼Œå‘çŽ°æ’­å¹¿å‘Šï¼Œå°±å…³äº†ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Turn on the TV, and itâ€™s just ads, so turn it off.\n",
            "\n",
            "ZH: æ”¶åˆ°é‚®ä»¶ï¼Œè¯»å®Œï¼Œç›´æŽ¥åˆ é™¤ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "We read it, we delete it.\n",
            "\n",
            "ZH: è·¯è¿‡ä¹¦åº—ï¼Œè¿›åŽ»é€›äº†åŠå°æ—¶ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "I walked by the bookstore, went in and hung out for 30 minutes.\n",
            "\n",
            "--- ZHâ†’EN  relationship_explicitness ---\n",
            "\n",
            "ZH: æˆ‘å“¥å“¥æ˜¨å¤©ç»“å©šã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "My brother got married yesterday.\n",
            "\n",
            "ZH: å¥¹å§å§åœ¨é“¶è¡Œå·¥ä½œã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Her sister works at the bank.\n",
            "\n",
            "ZH: æˆ‘å ‚å“¥åœ¨å›½å¤–å®šå±…ã€‚\n",
            "EN: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "My cousin lives abroad.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(save_dir)"
      ],
      "metadata": {
        "id": "22qKOpNCZ2fW"
      },
      "id": "22qKOpNCZ2fW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c251732b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c251732b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54092408-c2c9-45a6-ae07-1dad8e8349b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel, tokenizer = FastLanguageModel.from_pretrained(\\n    \"my-lora-adapter\",\\n    load_in_4bit=True,\\n)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# ===========================\n",
        "# save hf format LoRa adapter\n",
        "# ===========================\n",
        "model.save_pretrained(\"my-lora-adapter\")   # HF format\n",
        "tokenizer.save_pretrained(\"my-lora-adapter\")\n",
        "\n",
        "# load:\n",
        "\"\"\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"my-lora-adapter\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}