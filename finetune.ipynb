{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230a5984",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timz815/360-NLP-Project/blob/main/test_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bfd349",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen3-4B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7975c70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json, os\n",
    "\n",
    "jsonl_path = \"/content/drive/MyDrive/movie_dialogue.jsonl\"\n",
    "data = [json.loads(line) for line in open(jsonl_path, encoding=\"utf-8\")]\n",
    "\n",
    "def format_example(ex):\n",
    "    # Must match the keys in your JSONL\n",
    "    messages = [\n",
    "        {\"role\": \"user\",      \"content\": ex[\"english\"]},\n",
    "        {\"role\": \"assistant\", \"content\": ex[\"chinese\"]},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds = Dataset.from_list(data).map(format_example, remove_columns=data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9049f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63154389",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=20,\n",
    "        max_steps=300,          # increase for real run\n",
    "        logging_steps=1,\n",
    "        output_dir=\"/content/drive/MyDrive/qwen3-4b-dialogue-lora\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        num_train_epochs=1,     # or 2-3\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"/content/drive/MyDrive/qwen3-4b-dialogue-lora/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e3842",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#QUICK TEST\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"I’m gonna make him an offer he can’t refuse.\"}]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "out = model.generate(input_ids=inputs, max_new_tokens=128, temperature=0.7, do_sample=True)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251732b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\n",
    "    \"/content/drive/MyDrive/qwen3-4b-dialogue-lora/gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
